{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce165585",
   "metadata": {},
   "source": [
    "## Optimisation \n",
    "### Module 21 Challenge - Deep Learning - 2023 March"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b239427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "#  Import and read the charity_data.csv.\n",
    "import pandas as pd \n",
    "application_df = pd.read_csv(\"Resources/charity_data.csv\")\n",
    "application_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d866c6f",
   "metadata": {},
   "source": [
    "## Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71704244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.\n",
    "application_df = application_df.drop(['EIN', 'NAME'], axis=1)\n",
    "# Choose a cutoff value and create a list of application types to be replaced\n",
    "# use the variable name `application_types_to_replace`\n",
    "cutoff = 1000\n",
    "application_counts = application_df['APPLICATION_TYPE'].value_counts()\n",
    "application_types_to_replace = list(application_counts[application_counts < cutoff].index)\n",
    "# Replace in dataframe\n",
    "for app in application_types_to_replace:\n",
    "    application_df['APPLICATION_TYPE'] = application_df['APPLICATION_TYPE'].replace(app,\"Other\")\n",
    "# Group low-frequency AFFILIATION categories\n",
    "low_freq_categories = ['Family/Parent', 'National', 'Regional', 'Other']\n",
    "application_df['AFFILIATION'] = application_df['AFFILIATION'].apply(lambda x: 'Other' if x in low_freq_categories else x)\n",
    "# Choose a cutoff value and create a list of classifications to be replaced\n",
    "# use the variable name `classifications_to_replace`\n",
    "cutoff = 1000\n",
    "classification_counts = application_df['CLASSIFICATION'].value_counts()\n",
    "classifications_to_replace = list(classification_counts[classification_counts < cutoff].index)\n",
    "# Replace in dataframe\n",
    "for cls in classifications_to_replace:\n",
    "    application_df['CLASSIFICATION'] = application_df['CLASSIFICATION'].replace(cls,\"Other\")\n",
    "# Group low-frequency USE_CASE categories\n",
    "low_freq_categories = ['CommunityServ', 'Heathcare', 'Other']\n",
    "application_df['USE_CASE'] = application_df['USE_CASE'].apply(lambda x: 'Other' if x in low_freq_categories else x)\n",
    "# Group low-frequency ORGANIZATION categories\n",
    "low_freq_categories = ['Co-operative', 'Corporation']\n",
    "application_df['ORGANIZATION'] = application_df['ORGANIZATION'].apply(lambda x: 'Other' if x in low_freq_categories else x)\n",
    "# Drop the STATUS Column\n",
    "application_df = application_df.drop(['STATUS'], axis=1)\n",
    "# Convert the INCOME_AMT column to a categorical variable\n",
    "income_categories = ['0', '1M-5M', '100000-499999', '10M-50M', '25000-99999', '5M-10M', '10000-24999', '50M+']\n",
    "application_df['INCOME_AMT'] = pd.Categorical(application_df['INCOME_AMT'], categories=income_categories, ordered=True)\n",
    "# Replace missing values with the mode\n",
    "income_mode = application_df['INCOME_AMT'].mode()[0]\n",
    "application_df['INCOME_AMT'].replace(['1/01/9999'], income_mode, inplace=True)\n",
    "# One-hot encode the INCOME_AMT column\n",
    "income_dummies = pd.get_dummies(application_df['INCOME_AMT'], prefix='INCOME')\n",
    "application_df = pd.concat([application_df, income_dummies], axis=1)\n",
    "# Drop the original INCOME_AMT column\n",
    "application_df.drop(['INCOME_AMT'], axis=1, inplace=True)\n",
    "# Drop the SPECIAL_CONSIDERATIONS Column\n",
    "application_df = application_df.drop(['SPECIAL_CONSIDERATIONS'], axis=1)\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "# Reshape the feature to a 2D array and fit the scaler to the data\n",
    "ask_amt = application_df[\"ASK_AMT\"].values.reshape(-1, 1)\n",
    "scaler.fit(ask_amt)\n",
    "# Transform the feature using the scaler\n",
    "scaled_ask_amt = scaler.transform(ask_amt)\n",
    "# Replace the original feature in the DataFrame with the scaled feature\n",
    "application_df[\"ASK_AMT\"] = scaled_ask_amt\n",
    "# Convert categorical data to numeric with `pd.get_dummies`\n",
    "application_df = pd.get_dummies(application_df, columns=['AFFILIATION','CLASSIFICATION', 'USE_CASE', 'ORGANIZATION', 'APPLICATION_TYPE'])\n",
    "application_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56d1c7e",
   "metadata": {},
   "source": [
    "### Split Preprocessed Data & Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b020ce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "y = application_df['IS_SUCCESSFUL'].values\n",
    "X = application_df.drop('IS_SUCCESSFUL', axis=1).values\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)\n",
    "\n",
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1773b337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hp):\n",
    "    nn_model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Allow kerastuner to decide which activation function to use in hidden layers\n",
    "    activation = hp.Choice('activation',['relu','tanh','sigmoid'])\n",
    "    \n",
    "    # Allow kerastuner to decide number of neurons in first layer\n",
    "    nn_model.add(tf.keras.layers.Dense(units=hp.Int('first_units',\n",
    "        min_value=1,\n",
    "        max_value=10,\n",
    "        step=2), activation=activation, input_dim=30))\n",
    "\n",
    "    # Allow kerastuner to decide number of hidden layers and neurons in hidden layers\n",
    "    for i in range(hp.Int('num_layers', 1, 6)):\n",
    "        nn_model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),\n",
    "            min_value=1,\n",
    "            max_value=10,\n",
    "            step=2),\n",
    "            activation=activation))\n",
    "    \n",
    "    nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "    # Compile the model\n",
    "    nn_model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "    \n",
    "    return nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfa0cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install keras-tuner --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a25b717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the kerastuner library\n",
    "import keras_tuner as kt\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    create_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_epochs=20,\n",
    "    hyperband_iterations=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339bee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the kerastuner search for best hyperparameters\n",
    "tuner.search(X_train_scaled,y_train,epochs=20,validation_data=(X_test_scaled,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa16f317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top hyperparameter and print the values\n",
    "top_hyper = tuner.get_best_hyperparameters(1)\n",
    "for param in top_hyper:\n",
    "    print(param.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa137638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Assume y_pred and y_true are the predicted and true binary labels, respectively\n",
    "y_pred_proba = nn_model.predict(X_test_scaled).ravel()\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93302101",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
